{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb4bec9",
   "metadata": {},
   "source": [
    "## Solving Utility Maximization Problem with Quantum Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b17c25",
   "metadata": {},
   "source": [
    "In general markets, the competitive equilibrium, or more generally, Dynamic Stochastic General Equilibrium (DSGE) is characterized by a set of state variables and the consumption and production plans of each agent to maximize the utility. Such utility maximization problem has been traditionally dealt with Lagrangian methods. In this Hackathon project, we demonstrate **a quantum approach to solving utility maximization problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dfa3d8",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4c1170",
   "metadata": {},
   "source": [
    "[1] Hill, Edward, Marco Bardoscia, and Arthur Turrell. \"Solving heterogeneous general equilibrium economic models with deep reinforcement learning.\" arXiv preprint [arXiv:2103.16977 (2021)](https://arxiv.org/pdf/2103.16977.pdf).\n",
    "\n",
    "[2] Wu, Shaojun, et al. \"Quantum reinforcement learning in continuous action space.\" arXiv preprint [arXiv:2012.10711 (2020)](https://arxiv.org/pdf/2012.10711.pdf).\n",
    "\n",
    "[3] https://github.com/LauraGentini/QRL (Qiskit Hackathon Europe on Quantum Reinforcement Learning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623136bd",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c499180",
   "metadata": {},
   "source": [
    "We first constructed a simplified model of DSGE problem based on the recent work done by researchers at University of College, London [1]. In our model, we assume a single, rational household agent, and a single firm existing in the market. The agent is employed by the firm, and the utility per period is given by $u_t=\\textrm{ln}(c_{t})-\\frac{\\theta}{2}n_t^2$, with the action variables $c_{t}$, representing consumption, and $n_t$, the number of hours worked. We use a discrete timestep over $0\\leq t<T$. The price is fixed for $p_{t}=1$ for all times, with no interest rate. The wage is given by $w=1$ for $t<T/2$ and $w=0.5$ afterwards. The agent is also subject to a budget constraint $b_{t+1}=b_t+w_tn_t-p_tc_t$, with the no Ponzi condition $b_T=0$ to preempt unlimited borrowing by the agent. The agent wants to maximize the dicsounted utility $\\sum_t \\beta^t u_t$ with $\\beta=0.97$ and $T=20$. To summarize:\n",
    "\n",
    "* A single agent and a single firm\n",
    "* Maximize $\\sum_{t=0}^{T} \\beta^t u_t$, where $u_t=\\textrm{ln}(c_{t})-\\frac{\\theta}{2}n_t^2$, $\\beta=0.97$ and $T=20$\n",
    "* Budget Constraints $b_{t+1}=b_t+w_tn_t-p_tc_t$; $b_T=0$, where $p_{t}=1$, $w=1$ or $0.5$\n",
    "\n",
    "There have been some attempts to solve this type of problem using classical machine learning or numerical analysis, but Quantum approach to such problems is absent. We applied Quantum Reinforcement Learning technique to solve the problem. Before we explain our solution, we first present some preliminaries on Reinforcement Learning necessary to understand our approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c80fe1",
   "metadata": {},
   "source": [
    "### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2308572e",
   "metadata": {},
   "source": [
    "In Reinforcement Learning problems, there are states, and actions that the agent perform at each time step. The agent is given some rewards upon performing each action. Our goal is to find a set of states and actions that maximize the total rewards over the entire period. To be more mathematically precise, we have to introduce the concept of [Markov Decision Process](https://en.wikipedia.org/wiki/Markov_decision_process) (MDP). A MDP problem is completely characterized by $(S,A,R,P,\\gamma)$, where $S$ is the state space, $A$ is the action space, $R$ represents a reward as a function of the current state and the chosen action, $P$ represents a transition probability given (state, action), and $\\gamma$ is the discount factor. The objective function we want to maximize is the value function, the expectation value of total rewards over all times, which is given as follows:\n",
    "$$V(s)=E\\biggl[\\sum_{t\\geq 0}\\gamma^t r_t|s_0=s\\biggr].$$\n",
    "\n",
    "While the value function is a measure of how preferrable each state is, it's actually more important to identify how preferaable each (state, action) pair is. The measure of such preferrability of (state,action) pair is given by the Q-value function, which is defined as\n",
    "$$Q(s)=E\\biggl[\\sum_{t\\geq 0}\\gamma^t r_t|s_0=s, a_0=a\\biggr].$$\n",
    "\n",
    "Machine Learning technique that attempts to systematically learn this Q-value function is known as Q-learning. Q-learning is also a method that we utilized in this hackathon project to solve our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274564ea",
   "metadata": {},
   "source": [
    "### Quantum Reinforcement Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97d16341",
   "metadata": {},
   "source": [
    "<div style=\"max-width:500px; margin: 0 auto;\">\n",
    "    <img  src=\"attachment:images/fig1.png\" width=\"500\"/>\n",
    "</div>\n",
    "<center>Figure1. Quantum Reinforcement Learning Model [1]</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd039da5",
   "metadata": {},
   "source": [
    "Quantum Reinforcement Learning model is virtually same as its classical counterpart, except for the fact that states are now represented and quantum states. As described in Figure1, there is an agent interacting with the environment, which is characterized by its state. The agent's action influences the environment, and this action is determined by the *policy*, which is chosen to maximize the expected reward over all times. The Quantum Circuit implementing this framework is shown below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eaf1859e",
   "metadata": {},
   "source": [
    "<div style=\"max-width:500px; margin: 0 auto;\">\n",
    "    <img  src=\"attachment:images/fig2.png\" width=\"500\"/>\n",
    "</div>\n",
    "<center>Figure2. Quantum Circuit for QRL model [1]</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88384513",
   "metadata": {},
   "source": [
    "In the above, the reward is calculated through a proper choice of $U_r$ and the measurement operator $M$, and the transition of states over time is given by $\\theta_t$, where $\\theta_t$ represents the agent's action. $\\theta_t$ is trained by a separate Quantum DDPG (Deep Deterministic Policy Gradient) algoritm. After enough amount of training on the policy determining the agent's action $\\theta_t$ given $s_t$, we're then equipped with the Quantum Neural Network to easily *tell* someone the best action to maximize their lifetime *reward* or *utility*. Pseudocode of Quantum DDPG algorithm is shown below [1]. Essentially, it's about training the Q-value function $Q_\\omega(s,\\theta)$ and the policy function $\\pi_\\eta(s)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6ba84ed",
   "metadata": {},
   "source": [
    "<div style=\"max-width:500px; margin: 0 auto;\">\n",
    "    <img  src=\"attachment:images/fig3.png\" width=\"500\"/>\n",
    "</div>\n",
    "<center>Figure3. Quantum DDPG algorithm [1]</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2367d4d",
   "metadata": {},
   "source": [
    "### Our solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd8c7f",
   "metadata": {},
   "source": [
    "As we're solving Quantum Reinforcement Learning problem, it's essential to define (1) rewards, (2) actions, and (3) states. Reward is straightforwardly given by the utility function. Actions, as specified in the problem statement, consist of consumption and the number of hours worked. We're now left with properly defining the states, and encoding these states into our Quantum Circuit. We first note that the agent's state is completely characterized by specifying $b_t$ and $c_t$ at all times, since $n_t$ could be calculated from the budget constraints. We now encode our state as $$ \\left| \\psi \\right\\rangle = \\textrm{cos}\\frac{\\theta}{2}\\left| 0 \\right\\rangle+e^{i\\phi}\\textrm{sin}\\frac{\\theta}{2}\\left| 1 \\right\\rangle,$$ where $b_t=\\textrm{tan}\\theta$ and $c_t=\\textrm{tan}{(\\phi/4)}$, with $-\\pi/2<\\theta<\\pi/2$ and $0\\leq\\phi\\leq2\\pi$. Through this encoding, we can encode the entirety of our state with a single qubit. Note that the reward is computed as the utility function without resorting to $U_r$ and $M$, and is assumed to be negative infinity for states that do NOT satisfy the budget constraint or $n_t\\geq0$. Encoding the state with such Bloch angles also makes the transition of states, $U(\\theta_t)$, easy to implement, as this transition can be implemented only with the rotations around $x,y,$ and $z$ axes on the Bloch sphere. With this Quantum Reinforcement Learning architecture and the encoding of states, we employed to Quantum DDPG algorithm to learn the Q-value function and the policy. In the final and last section of this notebook, we present our concrete implementation of Quantum Reinforcement Learning for solving our macroeconomic model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66eeea5",
   "metadata": {},
   "source": [
    "## Codes with Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de3092a",
   "metadata": {},
   "source": [
    "### Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9308ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Qiskit Circuit imports\n",
    "from qiskit.circuit import QuantumCircuit, QuantumRegister, Parameter, ParameterVector, ParameterExpression\n",
    "from qiskit.circuit.library import TwoLocal\n",
    "\n",
    "# Qiskit imports\n",
    "import qiskit as qk\n",
    "from qiskit.utils import QuantumInstance\n",
    "from qiskit import Aer\n",
    "from qiskit.tools.visualization import circuit_drawer\n",
    "from qiskit.quantum_info import state_fidelity\n",
    "from qiskit import BasicAer\n",
    "from qiskit.providers.aer.noise import NoiseModel, amplitude_damping_error\n",
    "\n",
    "\n",
    "# Qiskit Machine Learning imports\n",
    "import qiskit_machine_learning as qkml\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from math import pi\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a89de6",
   "metadata": {},
   "source": [
    "### Define sub-circuits for encoding states and training the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a4cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_circuit(inputs, num_qubits = 1, *args):\n",
    "    \"\"\"\n",
    "    Encode classical input data (i.e. the state of the enironment) on a quantum circuit. \n",
    "    To be used inside the `parametrized_circuit` function. \n",
    "    \n",
    "    Args\n",
    "    -------\n",
    "    inputs (list): a list containing the classical inputs.\n",
    "    num_qubits (int): number of qubits in the quantum circuit.\n",
    "    \n",
    "    Return\n",
    "    -------\n",
    "    qc (QuantumCircuit): quantum circuit with encoding gates.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    qc = qk.QuantumCircuit(num_qubits)\n",
    "    \n",
    "    # Encode data with a RX rotation\n",
    "    for i in range(num_qubits): \n",
    "        qc.u(inputs[i*2],inputs[i*2+1],0,i)\n",
    "        \n",
    "    return qc\n",
    "\n",
    "def action_circuit(num_qubits = 1,*args):\n",
    "    ac = qk.QuantumRegister(num_qubits)\n",
    "    qc = qk.QuantumCircuit(ac)\n",
    "    input = qk.circuit.ParameterVector('x', 4*num_qubits)\n",
    "    for i in range(num_qubits): \n",
    "        qc.rx(input[4*i], i)\n",
    "        qc.rz(input[4*i+1], i)\n",
    "        qc.u(input[4*i+2],input[4*i+3],0,i)\n",
    "    return qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5107375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parametrized_circuit(num_qubits = 1, reps = 1, insert_barriers = True, ):\n",
    "    \"\"\"\n",
    "    Create the Parameterized Quantum Circuit (PQC) for estimating Q-values.\n",
    "    It implements the architecure proposed in Skolik et al. arXiv:2104.15084.\n",
    "    \n",
    "    Args\n",
    "    -------\n",
    "    num_qubit (int): number of qubits in the quantum circuit. \n",
    "\n",
    "    reps (int): number of repetitions (layers) in the variational circuit. \n",
    "    insert_barrirerd (bool): True to add barriers in between gates, for better drawing of the circuit. \n",
    "\n",
    "    \n",
    "    Return\n",
    "    -------\n",
    "    qc (QuantumCircuit): the full parametrized quantum circuit. \n",
    "    \"\"\"\n",
    "    \n",
    "    qr = qk.QuantumRegister(num_qubits)\n",
    "    qc = qk.QuantumCircuit(qr)\n",
    "          \n",
    "    # Define a vector containg Inputs as parameters (*not* to be optimized)\n",
    "    inputs = qk.circuit.ParameterVector('x', 2*num_qubits)\n",
    "            \n",
    "    # Define a vector containng variational parameters\n",
    "    θ = qk.circuit.ParameterVector('θ', 3 * num_qubits * reps)\n",
    "    qc.compose(encoding_circuit(inputs, num_qubits = num_qubits), inplace = True)\n",
    "    if insert_barriers: qc.barrier()\n",
    "    \n",
    "    # Iterate for a number of repetitions\n",
    "    for rep in range(reps):\n",
    "\n",
    "        # Encode classical input data\n",
    "\n",
    "            \n",
    "        # Variational circuit (does the same as TwoLocal from Qiskit)\n",
    "        for qubit in range(num_qubits):\n",
    "            qc.rx(θ[qubit + 3*num_qubits*(rep)], qubit)\n",
    "            qc.rz(θ[qubit + 3*num_qubits*(rep) + num_qubits], qubit)\n",
    "            qc.rx(θ[qubit + 3*num_qubits*(rep) + 2*num_qubits], qubit)\n",
    "        if insert_barriers: qc.barrier()\n",
    "            \n",
    "        # Add entanglers (this code is for a circular entangler)\n",
    "        if num_qubits>2:\n",
    "            qc.cnot(qr[-1], qr[0])\n",
    "            for qubit in range(num_qubits-1):\n",
    "                qc.cnot(qr[qubit], qr[qubit+1])\n",
    "            if insert_barriers: qc.barrier()\n",
    "        elif num_qubits==2:\n",
    "            qc.cnot(qr[-1], qr[0])\n",
    "        \n",
    "    return qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff3ece2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 882.178x144.48 with 1 Axes>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the number of qubits\n",
    "num_qubits = 1\n",
    "\n",
    "# Generate the Parametrized Quantum Circuit (note the flags reuploading and reps)\n",
    "policy_qc = parametrized_circuit(num_qubits = num_qubits, \n",
    "                          reps = 2)\n",
    "\n",
    "value_qc=parametrized_circuit(num_qubits = 2*num_qubits, \n",
    "                          reps = 2)\n",
    "\n",
    "# Fetch the parameters from the circuit and divide them in Inputs (X) and Trainable Parameters (params)\n",
    "# The first four parameters are for the inputs \n",
    "policy_X = list(policy_qc.parameters)[: 2*num_qubits]\n",
    "value_X=list(value_qc.parameters)[: 4*num_qubits]\n",
    "\n",
    "# The remaining ones are the trainable weights of the quantum neural network\n",
    "policy_params = list(policy_qc.parameters)[num_qubits:]\n",
    "value_params=list(value_qc.parameters)[2*num_qubits:]\n",
    "\n",
    "policy_qc.draw('mpl')\n",
    "value_qc.draw('mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "632ba22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 327.397x84.28 with 1 Axes>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_qc=action_circuit(num_qubits=num_qubits)\n",
    "action_X=list(action_qc.parameters)\n",
    "action_qc.draw('mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f52556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a quantum backend to run the simulation of the quantum circuit\n",
    "qi = QuantumInstance(qk.BasicAer.get_backend('statevector_simulator'))\n",
    "\n",
    "# Create a Quantum Neural Network object starting from the quantum circuit defined above\n",
    "policy_qnn = CircuitQNN(policy_qc, input_params=policy_X, weight_params=policy_params, \n",
    "                 quantum_instance = qi)\n",
    "\n",
    "value_qnn = CircuitQNN(value_qc, input_params=value_X, weight_params=value_params, \n",
    "                 quantum_instance = qi)\n",
    "\n",
    "action_qnn= CircuitQNN(action_qc, input_params=action_X,quantum_instance = qi)\n",
    "\n",
    "\n",
    "policy_initial = 0.1*(2*torch.rand(policy_qnn.num_weights) - 1)\n",
    "policy_nn = TorchConnector(policy_qnn, policy_initial)\n",
    "\n",
    "value_initial = 0.1*(2*torch.rand(value_qnn.num_weights) - 1)\n",
    "value_nn = TorchConnector(value_qnn, value_initial)\n",
    "\n",
    "action_nn=TorchConnector(action_qnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2528f98e",
   "metadata": {},
   "source": [
    "### Implement the replay-memory technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc6995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_buffer():\n",
    "    def __init__(self ,max_size):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr=0\n",
    "        \n",
    "    def push(self,data):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        ind=np.random.randint(0,len(self.storage), size=batch_size)\n",
    "        k=0\n",
    "        for i in ind:\n",
    "            S, S_next, A, R, D =self.storage[i]\n",
    "            if k==0:\n",
    "                s=S\n",
    "                s_next=S_next\n",
    "                a=A\n",
    "                r=R\n",
    "                d=D\n",
    "                k+=1\n",
    "            else:\n",
    "\n",
    "                s=torch.vstack((s,S))\n",
    "                s_next=torch.vstack((s_next,S_next))\n",
    "                a=torch.vstack((a,A))\n",
    "                r=torch.vstack((r,R))\n",
    "                d=torch.vstack((d,D))    \n",
    "        return  s,s_next,a,r,d\n",
    "\n",
    "def getangle(s):\n",
    "    sizes=torch.abs(s)\n",
    "    return torch.tensor([2*torch.acos(sizes[0].type(torch.cfloat)),torch.atan2(torch.imag(s[1].type(torch.cfloat)),torch.real(s[1].type(torch.cfloat)))-torch.atan2(torch.imag(s[0].type(torch.cfloat)),torch.real(s[0].type(torch.cfloat)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2463362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class diffQ(nn.Module):\n",
    "    def __init__(self,valuef,policyf):\n",
    "        super().__init__()\n",
    "        self.value=valuef\n",
    "        self.policy=policyf\n",
    "        \n",
    "    def forward(self,s,a, currentq=True):\n",
    "        if currentq==True:\n",
    "            return self.value(s,a)\n",
    "        else:\n",
    "            return self.value(s,self.policy(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabfa27b",
   "metadata": {},
   "source": [
    "### Implement DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85af3cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(object):\n",
    "    def __init__(self,max_size=100,learning_rate=1e-3,batch_size=20,gamma=0.997,update_iteration=10,theta=1, tau=1e-3,noise=2*1e-2):\n",
    "        self.policy=policy_nn\n",
    "        self.policy_target=policy_nn\n",
    "        self.value=value_nn\n",
    "        self.value_target=value_nn\n",
    "        self.replay_buffer=Replay_buffer(max_size=max_size)\n",
    "        self.action_operator=action_nn\n",
    "        self.model= diffQ(self.value, self.policy)\n",
    "        self.model.train()\n",
    "        self.opt=optim.Adagrad(self.model.parameters(),lr=learning_rate)\n",
    "        self.inital=False\n",
    "        self.batch_size=batch_size\n",
    "        self.gamma=gamma\n",
    "        self.update_iteration=update_iteration\n",
    "        self.theta=theta\n",
    "        self.tau=tau\n",
    "        self.noise=noise\n",
    "\n",
    "    \n",
    "    def update(self):\n",
    "        for it in range(self.update_iteration):\n",
    "            s, s_next, a, r, d= self.replay_buffer.sample(batch_size=self.batch_size)\n",
    "            a = (a + torch.normal(0, self.noise, size= a.shape))\n",
    "            target_Q=[]\n",
    "            current_Q=[]\n",
    "            value_max=[]\n",
    "            for i in range(self.batch_size):\n",
    "                target_Q.append(self.value_target(torch.concat((getangle(s_next[i]).type(torch.float), getangle(self.policy_target(getangle(s_next[i]).type(torch.float))).type(torch.float))))[0])\n",
    "                current_Q.append(self.value(torch.concat((getangle(s[i]).type(torch.float), getangle(self.policy_target(getangle(s[i]).type(torch.float))).type(torch.float))))[0])\n",
    "                value_max.append(self.value(torch.concat((getangle(s[i]).type(torch.float), getangle(self.policy_target(getangle(s[i]).type(torch.float))).type(torch.float))))[0])\n",
    "            target_Q=torch.tensor(target_Q)\n",
    "            current_Q=torch.tensor(current_Q)\n",
    "            value_max=torch.tensor(value_max)\n",
    "            target_Q = r.type(torch.float)+ self.gamma*target_Q\n",
    "            mse_Q = F.mse_loss(current_Q, target_Q)\n",
    "            mse_Q.requires_grad=True\n",
    "            value_max = - torch.mean(value_max)\n",
    "            value_max.requires_grad=True\n",
    "            self.opt.zero_grad()\n",
    "            mse_Q.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "            value_max.backward()\n",
    "            self.opt.step()\n",
    "            \n",
    "            for param, target_param in zip(self.policy.parameters(), self.policy_target.parameters()):\n",
    "                target_param.data.copy_(self.tau*param.data+(1-self.tau)*target_param)\n",
    "            for param, target_param in zip(self.value.parameters(), self.value_target.parameters()):\n",
    "                target_param.data.copy_(self.tau*param.data+(1-self.tau)*target_param)\n",
    "                \n",
    "    def onetime(self,s,t):\n",
    "        d=torch.zeros((1,))\n",
    "        a=self.policy(getangle(s).type(torch.float))\n",
    "        actioninput=torch.concat((getangle(s).type(torch.float),getangle(a).type(torch.float)))\n",
    "        s_next=self.action_operator(actioninput)\n",
    "        angle=getangle(s)\n",
    "        angle_next=getangle(s_next)\n",
    "        \n",
    "        consume=F.relu((torch.tan(angle[0]-pi/2)-torch.tan(angle_next[0]-pi/2)).type(torch.float))+torch.tan(angle_next[1]/4)\n",
    "        \n",
    "        labor=-(torch.tan((angle[0]-pi/2))-torch.tan((angle_next[0]-pi/2)))+consume\n",
    "        \n",
    "        if t>10:\n",
    "            labor=labor*2\n",
    "        r=torch.log(1e-30+consume.type(torch.float))-(self.theta*labor**2)/2    \n",
    "        if t==19:\n",
    "            if torch.abs(angle_next[0])**2>0.0001:\n",
    "                r=torch.tensor([-100])\n",
    "                d=d+1\n",
    "            return [s,s_next,a,r,d]\n",
    "        else:\n",
    "            return [s,s_next,a,r,d]\n",
    "        \n",
    "    def push_buffer(self,s0):\n",
    "        s=s0\n",
    "        reward=0\n",
    "        for t in range(20):\n",
    "            data=self.onetime(s,t)\n",
    "            self.replay_buffer.push(data)\n",
    "            s=data[1]\n",
    "            reward=reward+data[3]\n",
    "        print(reward)\n",
    "        return reward    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65dd02d",
   "metadata": {},
   "source": [
    "### Run DDPG algorithm to train the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382637dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=DDPG()\n",
    "reward=[]\n",
    "for i in range(5):\n",
    "    reward.append(a.push_buffer(torch.tensor([1/sqrt(2),1/sqrt(2)])))\n",
    "for epoch in range(100):\n",
    "    a.update()\n",
    "    reward.append(a.push_buffer(torch.tensor([1/sqrt(2),1/sqrt(2)])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernel_torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
